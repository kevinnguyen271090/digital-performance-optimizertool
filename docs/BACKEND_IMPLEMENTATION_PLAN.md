# BACKEND IMPLEMENTATION PLAN

## üéØ M·ª•c ti√™u
X√¢y d·ª±ng backend ho√†n ch·ªânh theo ki·∫øn tr√∫c h·ªá th·ªëng ƒë√£ ƒë·ªÅ xu·∫•t: Python FastAPI + Cron Jobs + Data Pipeline + AI Insights.

## üìã K·∫ø ho·∫°ch tri·ªÉn khai

### **Phase 1: Core Backend API (∆Øu ti√™n cao)**

#### 1.1 Python FastAPI Backend
```bash
# T·∫°o th∆∞ m·ª•c backend
mkdir backend
cd backend

# C√†i ƒë·∫∑t dependencies
pip install fastapi uvicorn python-dotenv sqlalchemy psycopg2-binary
pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client
pip install facebook-business python-dateutil pandas numpy
pip install celery redis python-jose[cryptography] passlib[bcrypt]
```

#### 1.2 C·∫•u tr√∫c Backend (ƒê√£ c·∫≠p nh·∫≠t 2025)
```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI app entrypoint
‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Core config, security, celery
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ security.py
‚îÇ   ‚îú‚îÄ‚îÄ database/               # DB connection, session, migrations
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ organization.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ goals.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/                # Pydantic schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ goals.py
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ goals.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ organizations.py
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Business logic/service layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google_analytics.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ meta_ads.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ woocommerce.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai_insights.py
‚îÇ   ‚îú‚îÄ‚îÄ tasks/                  # Celery tasks, scheduled jobs
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Helper functions, utilities
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ env.example
```

#### 1.3 API Endpoints c·∫ßn x√¢y d·ª±ng
```python
# Analytics API
GET /api/analytics/dashboard/{org_id}
GET /api/analytics/channel/{channel_id}
POST /api/analytics/aggregate
GET /api/analytics/insights/{org_id}

# Goals API
GET /api/goals/{org_id}
POST /api/goals
PUT /api/goals/{goal_id}
DELETE /api/goals/{goal_id}

# Organizations API
GET /api/organizations
POST /api/organizations
PUT /api/organizations/{org_id}

# Data Sync API
POST /api/sync/google-analytics
POST /api/sync/meta-ads
POST /api/sync/woocommerce
```

### **Phase 2: Data Pipeline & Cron Jobs (∆Øu ti√™n cao)**

#### 2.1 Celery Configuration
```python
# backend/app/core/celery.py
from celery import Celery
from app.core.config import settings

celery_app = Celery(
    "digital_performance_optimizer",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
    include=[
        "app.tasks.google_analytics",
        "app.tasks.meta_ads", 
        "app.tasks.woocommerce",
        "app.tasks.ai_insights"
    ]
)
```

#### 2.2 Scheduled Tasks
```python
# backend/app/tasks/google_analytics.py
from app.core.celery import celery_app
from app.services.google_analytics import GoogleAnalyticsService

@celery_app.task
def fetch_google_analytics_data():
    """Fetch Google Analytics data every 15 minutes"""
    service = GoogleAnalyticsService()
    return service.fetch_and_store_data()

@celery_app.task
def aggregate_hourly_data():
    """Aggregate hourly data every hour"""
    # Logic t·ªïng h·ª£p d·ªØ li·ªáu theo gi·ªù

@celery_app.task  
def aggregate_daily_data():
    """Aggregate daily data at midnight"""
    # Logic t·ªïng h·ª£p d·ªØ li·ªáu theo ng√†y
```

#### 2.3 Cron Schedule
```python
# backend/app/core/celery.py
celery_app.conf.beat_schedule = {
    'fetch-google-analytics': {
        'task': 'app.tasks.google_analytics.fetch_google_analytics_data',
        'schedule': 900.0,  # 15 minutes
    },
    'fetch-meta-ads': {
        'task': 'app.tasks.meta_ads.fetch_meta_ads_data', 
        'schedule': 900.0,  # 15 minutes
    },
    'aggregate-hourly': {
        'task': 'app.tasks.analytics.aggregate_hourly_data',
        'schedule': 3600.0,  # 1 hour
    },
    'aggregate-daily': {
        'task': 'app.tasks.analytics.aggregate_daily_data',
        'schedule': crontab(hour=0, minute=0),  # Midnight
    },
    'generate-ai-insights': {
        'task': 'app.tasks.ai_insights.generate_insights',
        'schedule': 3600.0,  # 1 hour
    },
}
```

### **Phase 3: AI Insights Engine (∆Øu ti√™n trung b√¨nh)**

#### 3.1 AI Insights Service
```python
# backend/app/services/ai_insights.py
import pandas as pd
import numpy as np
from typing import Dict, List
import openai

class AIInsightsService:
    def __init__(self):
        self.openai_client = openai.OpenAI()
    
    def detect_anomalies(self, data: pd.DataFrame) -> List[Dict]:
        """Ph√°t hi·ªán b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu"""
        # Logic ph√°t hi·ªán b·∫•t th∆∞·ªùng
        
    def generate_recommendations(self, data: pd.DataFrame) -> List[Dict]:
        """T·∫°o g·ª£i √Ω t·ªëi ∆∞u marketing"""
        # Logic t·∫°o g·ª£i √Ω
        
    def predict_trends(self, data: pd.DataFrame) -> Dict:
        """D·ª± b√°o xu h∆∞·ªõng"""
        # Logic d·ª± b√°o
```

#### 3.2 Data Analysis Functions
```python
# backend/app/utils/analytics.py
def calculate_roas(revenue: float, cost: float) -> float:
    """T√≠nh ROAS"""
    return revenue / cost if cost > 0 else 0

def calculate_cpa(cost: float, conversions: int) -> float:
    """T√≠nh CPA"""
    return cost / conversions if conversions > 0 else 0

def detect_performance_changes(current: float, previous: float, threshold: float = 0.1) -> bool:
    """Ph√°t hi·ªán thay ƒë·ªïi hi·ªáu su·∫•t"""
    return abs(current - previous) / previous > threshold
```

### **Phase 4: Data Storage & Aggregation (∆Øu ti√™n cao)**

#### 4.1 Database Models
```python
# backend/app/models/analytics.py
from sqlalchemy import Column, String, Float, DateTime, JSON, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from app.database import Base

class HourlyAggregate(Base):
    __tablename__ = "hourly_aggregates"
    
    id = Column(UUID(as_uuid=True), primary_key=True)
    organization_id = Column(UUID(as_uuid=True), ForeignKey("organizations.id"))
    channel = Column(String)
    metric = Column(String)
    value = Column(Float)
    timestamp = Column(DateTime)
    metadata = Column(JSON)

class DailyAggregate(Base):
    __tablename__ = "daily_aggregates"
    
    id = Column(UUID(as_uuid=True), primary_key=True)
    organization_id = Column(UUID(as_uuid=True), ForeignKey("organizations.id"))
    channel = Column(String)
    metric = Column(String)
    value = Column(Float)
    date = Column(DateTime)
    metadata = Column(JSON)

class AIInsight(Base):
    __tablename__ = "ai_insights"
    
    id = Column(UUID(as_uuid=True), primary_key=True)
    organization_id = Column(UUID(as_uuid=True), ForeignKey("organizations.id"))
    insight_type = Column(String)  # anomaly, recommendation, trend
    title = Column(String)
    description = Column(String)
    severity = Column(String)  # low, medium, high
    created_at = Column(DateTime)
    metadata = Column(JSON)
```

#### 4.2 Data Aggregation Logic
```python
# backend/app/services/analytics.py
class AnalyticsService:
    def aggregate_hourly_data(self, org_id: str):
        """T·ªïng h·ª£p d·ªØ li·ªáu theo gi·ªù"""
        # Logic t·ªïng h·ª£p t·ª´ raw data sang hourly aggregates
        
    def aggregate_daily_data(self, org_id: str):
        """T·ªïng h·ª£p d·ªØ li·ªáu theo ng√†y"""
        # Logic t·ªïng h·ª£p t·ª´ hourly sang daily aggregates
        
    def cleanup_old_data(self):
        """D·ªçn d·∫πp d·ªØ li·ªáu c≈©"""
        # X√≥a hourly aggregates > 1 ng√†y
        # Backup daily aggregates > 365 ng√†y
```

### **Phase 5: Deployment & Infrastructure (∆Øu ti√™n trung b√¨nh)**

#### 5.1 Docker Configuration
```dockerfile
# backend/Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### 5.2 Docker Compose
```yaml
# backend/docker-compose.yml
version: '3.8'
services:
  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - redis
      
  celery:
    build: .
    command: celery -A app.core.celery worker --loglevel=info
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - redis
      
  celery-beat:
    build: .
    command: celery -A app.core.celery beat --loglevel=info
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - redis
      
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

## üöÄ Tri·ªÉn khai theo th·ª© t·ª± ∆∞u ti√™n

### **Tu·∫ßn 1-2: Core Backend API**
1. Setup Python FastAPI project
2. T·∫°o database models v√† schemas
3. Implement authentication v√† authorization
4. T·∫°o basic API endpoints

### **Tu·∫ßn 3-4: Data Pipeline**
1. Setup Celery v√† Redis
2. Implement Google Analytics service
3. Implement Meta Ads service
4. T·∫°o scheduled tasks

### **Tu·∫ßn 5-6: Data Aggregation**
1. Implement hourly/daily aggregation
2. Setup data cleanup processes
3. Test data pipeline end-to-end

### **Tu·∫ßn 7-8: AI Insights**
1. Implement anomaly detection
2. Implement recommendation engine
3. Test AI insights generation

### **Tu·∫ßn 9-10: Deployment**
1. Setup Docker containers
2. Deploy to cloud (DigitalOcean/AWS)
3. Monitor v√† optimize performance

## üí∞ Chi ph√≠ ∆∞·ªõc t√≠nh

### **Development Phase (2-3 th√°ng)**
- Backend Server: $20-50/th√°ng (DigitalOcean/AWS)
- Redis: $10-20/th√°ng
- Monitoring: $10-20/th√°ng
- **T·ªïng: $40-90/th√°ng**

### **Production Phase (1000+ users)**
- Backend Server: $50-100/th√°ng
- Redis: $20-40/th√°ng
- Database: $25-50/th√°ng (Supabase Pro)
- Monitoring: $20-40/th√°ng
- **T·ªïng: $115-230/th√°ng**

## üéØ K·∫øt qu·∫£ mong ƒë·ª£i

### **Sau khi ho√†n th√†nh Backend:**
- ‚úÖ Data pipeline t·ª± ƒë·ªông fetch d·ªØ li·ªáu marketing
- ‚úÖ AI insights t·ª± ƒë·ªông ph√°t hi·ªán b·∫•t th∆∞·ªùng v√† g·ª£i √Ω t·ªëi ∆∞u
- ‚úÖ Performance dashboard real-time v·ªõi d·ªØ li·ªáu th·ª±c
- ‚úÖ Scalable architecture cho 1000+ doanh nghi·ªáp
- ‚úÖ Chi ph√≠ v·∫≠n h√†nh <$200/th√°ng

### **Metrics c·∫£i thi·ªán:**
- Data freshness: T·ª´ manual ‚Üí Real-time (15 ph√∫t)
- Insights quality: T·ª´ static ‚Üí AI-powered
- User experience: T·ª´ demo ‚Üí Production-ready
- Scalability: T·ª´ single-user ‚Üí Multi-tenant enterprise 