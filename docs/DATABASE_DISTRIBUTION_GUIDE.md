# HÆ¯á»šNG DáºªN PHÃ‚N CHIA DATABASE: SUPABASE vs BIGQUERY

## ğŸ¯ Má»¥c tiÃªu
PhÃ¢n chia báº£ng giá»¯a Supabase (hot data) vÃ  BigQuery (cold storage) Ä‘á»ƒ tá»‘i Æ°u chi phÃ­ vÃ  hiá»‡u nÄƒng theo kiáº¿n trÃºc há»‡ thá»‘ng.

## ğŸ“Š PHÃ‚N CHIA THEO KIáº¾N TRÃšC Há»† THá»NG

### **ğŸ”¥ SUPABASE (Hot Data - Truy cáº­p thÆ°á»ng xuyÃªn)**

#### **1. Core Business Tables**
```sql
-- âœ… Táº I SUPABASE - Truy cáº­p thÆ°á»ng xuyÃªn
âœ… organizations
âœ… organization_members  
âœ… user_profiles
âœ… user_2fa
```

**LÃ½ do**: Quáº£n lÃ½ user, organization, authentication - truy cáº­p liÃªn tá»¥c.

#### **2. Real-time Analytics Tables**
```sql
-- âœ… Táº I SUPABASE - Real-time data
âœ… hourly_aggregates (1 ngÃ y)
âœ… daily_aggregates (365 ngÃ y)
âœ… realtime_sessions (24 giá»)
âœ… event_tracking (30 ngÃ y)
```

**LÃ½ do**: Dashboard real-time, performance monitoring, user tracking.

#### **3. Active Insights Tables**
```sql
-- âœ… Táº I SUPABASE - Active insights
âœ… ai_insights (90 ngÃ y)
âœ… performance_alerts (30 ngÃ y)
âœ… funnel_analysis (365 ngÃ y)
âœ… cohort_analysis (365 ngÃ y)
```

**LÃ½ do**: AI insights, alerts, active analytics - cáº§n truy cáº­p nhanh.

#### **4. New Chart Tables**
```sql
-- âœ… Táº I SUPABASE - Chart data
âœ… demographics_data (365 ngÃ y)
âœ… creative_performance (365 ngÃ y)
âœ… competitive_data (365 ngÃ y)
```

**LÃ½ do**: Chart data cho dashboard - cáº§n query nhanh.

### **â„ï¸ BIGQUERY (Cold Storage - Dá»¯ liá»‡u lÃ¢u dÃ i)**

#### **1. Historical Data Backup**
```sql
-- â„ï¸ Táº I BIGQUERY - Historical backup
â„ï¸ raw_data_backup_historical (>365 ngÃ y)
â„ï¸ hourly_aggregates_historical (>1 ngÃ y)
â„ï¸ daily_aggregates_historical (>365 ngÃ y)
â„ï¸ ai_insights_historical (>90 ngÃ y)
```

**LÃ½ do**: Dá»¯ liá»‡u cÅ©, Ã­t truy cáº­p, chi phÃ­ tháº¥p.

#### **2. Deep Analytics Tables**
```sql
-- â„ï¸ Táº I BIGQUERY - Deep analysis
â„ï¸ long_term_cohort_analysis (>365 ngÃ y)
â„ï¸ historical_funnel_analysis (>365 ngÃ y)
â„ï¸ advanced_analytics_data (>1 nÄƒm)
â„ï¸ machine_learning_datasets (>2 nÄƒm)
```

**LÃ½ do**: PhÃ¢n tÃ­ch sÃ¢u, ML training, historical trends.

#### **3. Data Warehouse Tables**
```sql
-- â„ï¸ Táº I BIGQUERY - Data warehouse
â„ï¸ marketing_data_warehouse
â„ï¸ customer_journey_warehouse
â„ï¸ performance_metrics_warehouse
â„ï¸ competitive_intelligence_warehouse
```

**LÃ½ do**: Data warehouse cho business intelligence.

## ğŸš€ IMPLEMENTATION STRATEGY

### **Phase 1: MVP (1000 users)**
```sql
-- Táº¤T Cáº¢ Táº I SUPABASE
âœ… organizations
âœ… organization_members
âœ… user_profiles
âœ… hourly_aggregates (1 ngÃ y)
âœ… daily_aggregates (365 ngÃ y)
âœ… ai_insights (90 ngÃ y)
âœ… performance_alerts (30 ngÃ y)
âœ… realtime_sessions (24 giá»)
âœ… event_tracking (30 ngÃ y)
âœ… funnel_analysis (365 ngÃ y)
âœ… cohort_analysis (365 ngÃ y)
âœ… demographics_data (365 ngÃ y)
âœ… creative_performance (365 ngÃ y)
âœ… competitive_data (365 ngÃ y)
```

**Chi phÃ­**: ~$25/thÃ¡ng (Supabase Pro)

### **Phase 2: Scale (10k+ users)**
```sql
-- SUPABASE (Hot Data)
âœ… organizations
âœ… organization_members
âœ… user_profiles
âœ… hourly_aggregates (1 ngÃ y)
âœ… daily_aggregates (365 ngÃ y)
âœ… ai_insights (90 ngÃ y)
âœ… performance_alerts (30 ngÃ y)
âœ… realtime_sessions (24 giá»)
âœ… event_tracking (30 ngÃ y)
âœ… funnel_analysis (365 ngÃ y)
âœ… cohort_analysis (365 ngÃ y)
âœ… demographics_data (365 ngÃ y)
âœ… creative_performance (365 ngÃ y)
âœ… competitive_data (365 ngÃ y)

-- BIGQUERY (Cold Storage)
â„ï¸ raw_data_backup_historical
â„ï¸ hourly_aggregates_historical
â„ï¸ daily_aggregates_historical
â„ï¸ ai_insights_historical
â„ï¸ long_term_cohort_analysis
â„ï¸ historical_funnel_analysis
â„ï¸ marketing_data_warehouse
```

**Chi phÃ­**: ~$50/thÃ¡ng (Supabase + BigQuery)

### **Phase 3: Enterprise (100k+ users)**
```sql
-- SUPABASE (Hot Data - 30 ngÃ y)
âœ… organizations
âœ… organization_members
âœ… user_profiles
âœ… hourly_aggregates (1 ngÃ y)
âœ… daily_aggregates (30 ngÃ y)
âœ… ai_insights (30 ngÃ y)
âœ… performance_alerts (30 ngÃ y)
âœ… realtime_sessions (24 giá»)
âœ… event_tracking (30 ngÃ y)
âœ… funnel_analysis (30 ngÃ y)
âœ… cohort_analysis (30 ngÃ y)
âœ… demographics_data (30 ngÃ y)
âœ… creative_performance (30 ngÃ y)
âœ… competitive_data (30 ngÃ y)

-- BIGQUERY (Cold Storage + Data Warehouse)
â„ï¸ raw_data_backup_historical
â„ï¸ hourly_aggregates_historical
â„ï¸ daily_aggregates_historical (>30 ngÃ y)
â„ï¸ ai_insights_historical
â„ï¸ long_term_cohort_analysis
â„ï¸ historical_funnel_analysis
â„ï¸ marketing_data_warehouse
â„ï¸ customer_journey_warehouse
â„ï¸ performance_metrics_warehouse
â„ï¸ competitive_intelligence_warehouse
â„ï¸ machine_learning_datasets
â„ï¸ advanced_analytics_data
```

**Chi phÃ­**: ~$200/thÃ¡ng (Supabase + BigQuery + Advanced Analytics)

## ğŸ“‹ SETUP SCRIPTS

### **Supabase Setup Scripts**
```sql
-- File: scripts/setup-supabase-tables.sql

-- Core Business Tables
CREATE TABLE organizations (...);
CREATE TABLE organization_members (...);
CREATE TABLE user_profiles (...);
CREATE TABLE user_2fa (...);

-- Real-time Analytics Tables
CREATE TABLE hourly_aggregates (...);
CREATE TABLE daily_aggregates (...);
CREATE TABLE realtime_sessions (...);
CREATE TABLE event_tracking (...);

-- Active Insights Tables
CREATE TABLE ai_insights (...);
CREATE TABLE performance_alerts (...);
CREATE TABLE funnel_analysis (...);
CREATE TABLE cohort_analysis (...);

-- Chart Data Tables
CREATE TABLE demographics_data (...);
CREATE TABLE creative_performance (...);
CREATE TABLE competitive_data (...);
```

### **BigQuery Setup Scripts**
```sql
-- File: scripts/setup-bigquery-tables.sql

-- Historical Data Backup
CREATE TABLE raw_data_backup_historical (
    id STRING,
    organization_id STRING,
    source STRING,
    raw_data STRING,
    fetched_at TIMESTAMP,
    processed BOOL,
    created_at TIMESTAMP
);

CREATE TABLE hourly_aggregates_historical (
    id STRING,
    organization_id STRING,
    channel STRING,
    metric STRING,
    value FLOAT64,
    timestamp TIMESTAMP,
    metadata STRING,
    created_at TIMESTAMP
);

CREATE TABLE daily_aggregates_historical (
    id STRING,
    organization_id STRING,
    channel STRING,
    metric STRING,
    value FLOAT64,
    date DATE,
    metadata STRING,
    created_at TIMESTAMP
);

-- Deep Analytics Tables
CREATE TABLE long_term_cohort_analysis (
    id STRING,
    organization_id STRING,
    cohort_date DATE,
    cohort_type STRING,
    period_number INT64,
    period_type STRING,
    cohort_size INT64,
    active_users INT64,
    retention_rate FLOAT64,
    revenue FLOAT64,
    created_at TIMESTAMP
);

CREATE TABLE historical_funnel_analysis (
    id STRING,
    organization_id STRING,
    funnel_name STRING,
    step_name STRING,
    step_order INT64,
    date DATE,
    visitors INT64,
    conversions INT64,
    conversion_rate FLOAT64,
    drop_off_rate FLOAT64,
    created_at TIMESTAMP
);

-- Data Warehouse Tables
CREATE TABLE marketing_data_warehouse (
    id STRING,
    organization_id STRING,
    date DATE,
    channel STRING,
    campaign_id STRING,
    ad_group_id STRING,
    impressions INT64,
    clicks INT64,
    ctr FLOAT64,
    cpc FLOAT64,
    spend FLOAT64,
    conversions INT64,
    revenue FLOAT64,
    roas FLOAT64,
    created_at TIMESTAMP
);

CREATE TABLE customer_journey_warehouse (
    id STRING,
    organization_id STRING,
    user_id STRING,
    session_id STRING,
    touchpoint STRING,
    timestamp TIMESTAMP,
    channel STRING,
    campaign STRING,
    action STRING,
    value FLOAT64,
    created_at TIMESTAMP
);
```

## ğŸ”„ DATA PIPELINE STRATEGY

### **Supabase â†’ BigQuery Sync**
```python
# File: backend/app/tasks/data_sync.py

from google.cloud import bigquery
from supabase import create_client
import pandas as pd

def sync_supabase_to_bigquery():
    """Sync data tá»« Supabase sang BigQuery"""
    
    # 1. Láº¥y dá»¯ liá»‡u cÅ© tá»« Supabase
    supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    
    # 2. XÃ³a dá»¯ liá»‡u cÅ© khá»i Supabase
    def cleanup_old_data():
        # XÃ³a hourly_aggregates > 1 ngÃ y
        supabase.table('hourly_aggregates').delete().gte('timestamp', '1 day ago').execute()
        
        # XÃ³a daily_aggregates > 365 ngÃ y
        supabase.table('daily_aggregates').delete().gte('date', '365 days ago').execute()
        
        # XÃ³a ai_insights > 90 ngÃ y
        supabase.table('ai_insights').delete().gte('created_at', '90 days ago').execute()
    
    # 3. Upload lÃªn BigQuery
    def upload_to_bigquery(table_name, data):
        client = bigquery.Client()
        dataset_ref = client.dataset('digital_marketing')
        table_ref = dataset_ref.table(f'{table_name}_historical')
        
        # Convert to DataFrame vÃ  upload
        df = pd.DataFrame(data)
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
        )
        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()
    
    # 4. Schedule sync
    # Cháº¡y má»—i ngÃ y lÃºc 02:00
    # Cháº¡y má»—i tuáº§n cho historical data
```

### **BigQuery â†’ Supabase Sync (Khi cáº§n)**
```python
# File: backend/app/tasks/bigquery_sync.py

def sync_bigquery_to_supabase():
    """Sync data tá»« BigQuery vá» Supabase khi cáº§n"""
    
    client = bigquery.Client()
    
    # Query historical data
    query = """
    SELECT * FROM `project.dataset.daily_aggregates_historical`
    WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    """
    
    df = client.query(query).to_dataframe()
    
    # Upload vá» Supabase cho analysis
    supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    supabase.table('daily_aggregates').upsert(df.to_dict('records')).execute()
```

## ğŸ’° COST OPTIMIZATION

### **Supabase Costs**
```bash
# Free Tier (500MB, 2GB bandwidth)
âœ… 1000 users: Free
âœ… 5000 users: $25/thÃ¡ng (Pro)

# Pro Plan ($25/thÃ¡ng)
âœ… 8GB database
âœ… 250GB bandwidth
âœ… 1000+ organizations
```

### **BigQuery Costs**
```bash
# Storage (Long-term)
â„ï¸ $0.02/GB/thÃ¡ng (Standard)
â„ï¸ $0.004/GB/thÃ¡ng (Long-term)

# Query Processing
â„ï¸ $5/TB processed
â„ï¸ 1TB free/thÃ¡ng

# Estimated costs cho 1000 organizations
â„ï¸ Storage: ~$10/thÃ¡ng (500GB)
â„ï¸ Query: ~$5/thÃ¡ng (1TB processed)
â„ï¸ Total: ~$15/thÃ¡ng
```

### **Total Cost Estimation**
```bash
# Phase 1 (MVP - 1000 users)
âœ… Supabase Pro: $25/thÃ¡ng
â„ï¸ BigQuery: $0/thÃ¡ng (chÆ°a cáº§n)
ğŸ“Š Total: $25/thÃ¡ng

# Phase 2 (Scale - 10k users)
âœ… Supabase Pro: $25/thÃ¡ng
â„ï¸ BigQuery: $15/thÃ¡ng
ğŸ“Š Total: $40/thÃ¡ng

# Phase 3 (Enterprise - 100k users)
âœ… Supabase Pro: $25/thÃ¡ng
â„ï¸ BigQuery: $50/thÃ¡ng
â„ï¸ Advanced Analytics: $100/thÃ¡ng
ğŸ“Š Total: $175/thÃ¡ng
```

## ğŸ”§ MIGRATION STRATEGY

### **Phase 1: MVP (Hiá»‡n táº¡i)**
```sql
-- Táº¤T Cáº¢ Táº I SUPABASE
âœ… KhÃ´ng cáº§n BigQuery
âœ… Chi phÃ­ tháº¥p: $25/thÃ¡ng
âœ… Setup Ä‘Æ¡n giáº£n
âœ… Performance tá»‘t
```

### **Phase 2: Scale (Khi cáº§n)**
```sql
-- ThÃªm BigQuery khi:
â„ï¸ Dá»¯ liá»‡u > 8GB
â„ï¸ Cáº§n historical analysis
â„ï¸ Cáº§n advanced analytics
â„ï¸ Cáº§n ML training data
```

### **Migration Scripts**
```python
# File: scripts/migrate_to_bigquery.py

def migrate_to_bigquery():
    """Migrate data tá»« Supabase sang BigQuery"""
    
    # 1. Backup current data
    backup_supabase_data()
    
    # 2. Setup BigQuery
    setup_bigquery_tables()
    
    # 3. Sync data
    sync_supabase_to_bigquery()
    
    # 4. Update application config
    update_app_config()
    
    # 5. Test migration
    test_migration()
```

## ğŸ“Š MONITORING & ALERTS

### **Supabase Monitoring**
```sql
-- Monitor Supabase usage
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

### **BigQuery Monitoring**
```sql
-- Monitor BigQuery usage
SELECT 
    project_id,
    dataset_id,
    table_id,
    size_bytes,
    row_count
FROM `region-us`.INFORMATION_SCHEMA.TABLES
WHERE table_schema = 'digital_marketing'
ORDER BY size_bytes DESC;
```

## ğŸ¯ RECOMMENDATIONS

### **âœ… Báº¯t Ä‘áº§u vá»›i Supabase**
1. **Setup táº¥t cáº£ báº£ng táº¡i Supabase**
2. **Focus vÃ o performance vÃ  user experience**
3. **Monitor usage vÃ  costs**
4. **Plan migration khi cáº§n**

### **â„ï¸ ThÃªm BigQuery khi:**
1. **Dá»¯ liá»‡u > 8GB**
2. **Cáº§n historical analysis > 1 nÄƒm**
3. **Cáº§n advanced analytics**
4. **Cáº§n ML training data**
5. **CÃ³ budget cho enterprise features**

### **ğŸ”„ Migration Timeline**
```bash
# Month 1-6: Supabase only
âœ… Setup táº¥t cáº£ báº£ng táº¡i Supabase
âœ… Monitor performance vÃ  costs
âœ… Optimize queries vÃ  indexes

# Month 6-12: Evaluate BigQuery
â„ï¸ Setup BigQuery cho historical data
â„ï¸ Implement data sync pipeline
â„ï¸ Test hybrid approach

# Month 12+: Full hybrid
â„ï¸ Hot data: Supabase
â„ï¸ Cold data: BigQuery
â„ï¸ Advanced analytics: BigQuery
```

## ğŸ“‹ CHECKLIST

### **Supabase Setup**
- [ ] Core business tables
- [ ] Real-time analytics tables
- [ ] Active insights tables
- [ ] Chart data tables
- [ ] RLS policies
- [ ] Indexes optimization
- [ ] Functions vÃ  triggers

### **BigQuery Setup (Khi cáº§n)**
- [ ] Historical data tables
- [ ] Deep analytics tables
- [ ] Data warehouse tables
- [ ] Data sync pipeline
- [ ] Cost monitoring
- [ ] Performance optimization

### **Migration Planning**
- [ ] Data backup strategy
- [ ] Sync pipeline setup
- [ ] Application config updates
- [ ] Testing procedures
- [ ] Rollback plan

---

**Káº¿t luáº­n: Báº¯t Ä‘áº§u vá»›i Supabase cho táº¥t cáº£ báº£ng, thÃªm BigQuery khi cáº§n historical analysis vÃ  advanced analytics. Chi phÃ­ tá»‘i Æ°u vÃ  performance tá»‘t cho MVP.** 