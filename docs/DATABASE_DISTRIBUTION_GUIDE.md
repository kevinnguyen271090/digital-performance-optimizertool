# H∆Ø·ªöNG D·∫™N PH√ÇN CHIA DATABASE: SUPABASE vs BIGQUERY

## üéØ M·ª•c ti√™u
Ph√¢n chia b·∫£ng gi·ªØa Supabase (hot data) v√† BigQuery (cold storage) ƒë·ªÉ t·ªëi ∆∞u chi ph√≠ v√† hi·ªáu nƒÉng theo ki·∫øn tr√∫c h·ªá th·ªëng.

## üìä PH√ÇN CHIA THEO KI·∫æN TR√öC H·ªÜ TH·ªêNG

### **üî• SUPABASE (Hot Data - Truy c·∫≠p th∆∞·ªùng xuy√™n)**

#### **1. Core Business Tables**
```sql
-- ‚úÖ T·∫†I SUPABASE - Truy c·∫≠p th∆∞·ªùng xuy√™n
‚úÖ organizations
‚úÖ organization_members  
‚úÖ user_profiles
‚úÖ user_2fa
```

**L√Ω do**: Qu·∫£n l√Ω user, organization, authentication - truy c·∫≠p li√™n t·ª•c.

#### **2. Real-time Analytics Tables**
```sql
-- ‚úÖ T·∫†I SUPABASE - Real-time data
‚úÖ hourly_aggregates (1 ng√†y)
‚úÖ daily_aggregates (365 ng√†y)
‚úÖ realtime_sessions (24 gi·ªù)
‚úÖ event_tracking (30 ng√†y)
```

**L√Ω do**: Dashboard real-time, performance monitoring, user tracking.

#### **3. Active Insights Tables**
```sql
-- ‚úÖ T·∫†I SUPABASE - Active insights
‚úÖ ai_insights (90 ng√†y)
‚úÖ performance_alerts (30 ng√†y)
‚úÖ funnel_analysis (365 ng√†y)
‚úÖ cohort_analysis (365 ng√†y)
```

**L√Ω do**: AI insights, alerts, active analytics - c·∫ßn truy c·∫≠p nhanh.

#### **4. New Chart Tables**
```sql
-- ‚úÖ T·∫†I SUPABASE - Chart data
‚úÖ demographics_data (365 ng√†y)
‚úÖ creative_performance (365 ng√†y)
‚úÖ competitive_data (365 ng√†y)
```

**L√Ω do**: Chart data cho dashboard - c·∫ßn query nhanh.

### **‚ùÑÔ∏è BIGQUERY (Cold Storage - D·ªØ li·ªáu l√¢u d√†i)**

#### **1. Historical Data Backup**
```sql
-- ‚ùÑÔ∏è T·∫†I BIGQUERY - Historical backup
‚ùÑÔ∏è raw_data_backup_historical (>365 ng√†y)
‚ùÑÔ∏è hourly_aggregates_historical (>1 ng√†y)
‚ùÑÔ∏è daily_aggregates_historical (>365 ng√†y)
‚ùÑÔ∏è ai_insights_historical (>90 ng√†y)
```

**L√Ω do**: D·ªØ li·ªáu c≈©, √≠t truy c·∫≠p, chi ph√≠ th·∫•p.

#### **2. Deep Analytics Tables**
```sql
-- ‚ùÑÔ∏è T·∫†I BIGQUERY - Deep analysis
‚ùÑÔ∏è long_term_cohort_analysis (>365 ng√†y)
‚ùÑÔ∏è historical_funnel_analysis (>365 ng√†y)
‚ùÑÔ∏è advanced_analytics_data (>1 nƒÉm)
‚ùÑÔ∏è machine_learning_datasets (>2 nƒÉm)
```

**L√Ω do**: Ph√¢n t√≠ch s√¢u, ML training, historical trends.

#### **3. Data Warehouse Tables**
```sql
-- ‚ùÑÔ∏è T·∫†I BIGQUERY - Data warehouse
‚ùÑÔ∏è marketing_data_warehouse
‚ùÑÔ∏è customer_journey_warehouse
‚ùÑÔ∏è performance_metrics_warehouse
‚ùÑÔ∏è competitive_intelligence_warehouse
```

**L√Ω do**: Data warehouse cho business intelligence.

## üöÄ IMPLEMENTATION STRATEGY

### **Phase 1: MVP (1000 users)**
```sql
-- T·∫§T C·∫¢ T·∫†I SUPABASE
‚úÖ organizations
‚úÖ organization_members
‚úÖ user_profiles
‚úÖ hourly_aggregates (1 ng√†y)
‚úÖ daily_aggregates (365 ng√†y)
‚úÖ ai_insights (90 ng√†y)
‚úÖ performance_alerts (30 ng√†y)
‚úÖ realtime_sessions (24 gi·ªù)
‚úÖ event_tracking (30 ng√†y)
‚úÖ funnel_analysis (365 ng√†y)
‚úÖ cohort_analysis (365 ng√†y)
‚úÖ demographics_data (365 ng√†y)
‚úÖ creative_performance (365 ng√†y)
‚úÖ competitive_data (365 ng√†y)
```

**Chi ph√≠**: ~$25/th√°ng (Supabase Pro)

### **Phase 2: Scale (10k+ users)**
```sql
-- SUPABASE (Hot Data)
‚úÖ organizations
‚úÖ organization_members
‚úÖ user_profiles
‚úÖ hourly_aggregates (1 ng√†y)
‚úÖ daily_aggregates (365 ng√†y)
‚úÖ ai_insights (90 ng√†y)
‚úÖ performance_alerts (30 ng√†y)
‚úÖ realtime_sessions (24 gi·ªù)
‚úÖ event_tracking (30 ng√†y)
‚úÖ funnel_analysis (365 ng√†y)
‚úÖ cohort_analysis (365 ng√†y)
‚úÖ demographics_data (365 ng√†y)
‚úÖ creative_performance (365 ng√†y)
‚úÖ competitive_data (365 ng√†y)

-- BIGQUERY (Cold Storage)
‚ùÑÔ∏è raw_data_backup_historical
‚ùÑÔ∏è hourly_aggregates_historical
‚ùÑÔ∏è daily_aggregates_historical
‚ùÑÔ∏è ai_insights_historical
‚ùÑÔ∏è long_term_cohort_analysis
‚ùÑÔ∏è historical_funnel_analysis
‚ùÑÔ∏è marketing_data_warehouse
```

**Chi ph√≠**: ~$50/th√°ng (Supabase + BigQuery)

### **Phase 3: Enterprise (100k+ users)**
```sql
-- SUPABASE (Hot Data - 30 ng√†y)
‚úÖ organizations
‚úÖ organization_members
‚úÖ user_profiles
‚úÖ hourly_aggregates (1 ng√†y)
‚úÖ daily_aggregates (30 ng√†y)
‚úÖ ai_insights (30 ng√†y)
‚úÖ performance_alerts (30 ng√†y)
‚úÖ realtime_sessions (24 gi·ªù)
‚úÖ event_tracking (30 ng√†y)
‚úÖ funnel_analysis (30 ng√†y)
‚úÖ cohort_analysis (30 ng√†y)
‚úÖ demographics_data (30 ng√†y)
‚úÖ creative_performance (30 ng√†y)
‚úÖ competitive_data (30 ng√†y)

-- BIGQUERY (Cold Storage + Data Warehouse)
‚ùÑÔ∏è raw_data_backup_historical
‚ùÑÔ∏è hourly_aggregates_historical
‚ùÑÔ∏è daily_aggregates_historical (>30 ng√†y)
‚ùÑÔ∏è ai_insights_historical
‚ùÑÔ∏è long_term_cohort_analysis
‚ùÑÔ∏è historical_funnel_analysis
‚ùÑÔ∏è marketing_data_warehouse
‚ùÑÔ∏è customer_journey_warehouse
‚ùÑÔ∏è performance_metrics_warehouse
‚ùÑÔ∏è competitive_intelligence_warehouse
‚ùÑÔ∏è machine_learning_datasets
‚ùÑÔ∏è advanced_analytics_data
```

**Chi ph√≠**: ~$200/th√°ng (Supabase + BigQuery + Advanced Analytics)

## üìã SETUP SCRIPTS

### **Supabase Setup Scripts**
```sql
-- File: scripts/setup-supabase-tables.sql

-- Core Business Tables
CREATE TABLE organizations (...);
CREATE TABLE organization_members (...);
CREATE TABLE user_profiles (...);
CREATE TABLE user_2fa (...);

-- Real-time Analytics Tables
CREATE TABLE hourly_aggregates (...);
CREATE TABLE daily_aggregates (...);
CREATE TABLE realtime_sessions (...);
CREATE TABLE event_tracking (...);

-- Active Insights Tables
CREATE TABLE ai_insights (...);
CREATE TABLE performance_alerts (...);
CREATE TABLE funnel_analysis (...);
CREATE TABLE cohort_analysis (...);

-- Chart Data Tables
CREATE TABLE demographics_data (...);
CREATE TABLE creative_performance (...);
CREATE TABLE competitive_data (...);
```

### **BigQuery Setup Scripts**
```sql
-- File: scripts/setup-bigquery-tables.sql

-- Historical Data Backup
CREATE TABLE raw_data_backup_historical (
    id STRING,
    organization_id STRING,
    source STRING,
    raw_data STRING,
    fetched_at TIMESTAMP,
    processed BOOL,
    created_at TIMESTAMP
);

CREATE TABLE hourly_aggregates_historical (
    id STRING,
    organization_id STRING,
    channel STRING,
    metric STRING,
    value FLOAT64,
    timestamp TIMESTAMP,
    metadata STRING,
    created_at TIMESTAMP
);

CREATE TABLE daily_aggregates_historical (
    id STRING,
    organization_id STRING,
    channel STRING,
    metric STRING,
    value FLOAT64,
    date DATE,
    metadata STRING,
    created_at TIMESTAMP
);

-- Deep Analytics Tables
CREATE TABLE long_term_cohort_analysis (
    id STRING,
    organization_id STRING,
    cohort_date DATE,
    cohort_type STRING,
    period_number INT64,
    period_type STRING,
    cohort_size INT64,
    active_users INT64,
    retention_rate FLOAT64,
    revenue FLOAT64,
    created_at TIMESTAMP
);

CREATE TABLE historical_funnel_analysis (
    id STRING,
    organization_id STRING,
    funnel_name STRING,
    step_name STRING,
    step_order INT64,
    date DATE,
    visitors INT64,
    conversions INT64,
    conversion_rate FLOAT64,
    drop_off_rate FLOAT64,
    created_at TIMESTAMP
);

-- Data Warehouse Tables
CREATE TABLE marketing_data_warehouse (
    id STRING,
    organization_id STRING,
    date DATE,
    channel STRING,
    campaign_id STRING,
    ad_group_id STRING,
    impressions INT64,
    clicks INT64,
    ctr FLOAT64,
    cpc FLOAT64,
    spend FLOAT64,
    conversions INT64,
    revenue FLOAT64,
    roas FLOAT64,
    created_at TIMESTAMP
);

CREATE TABLE customer_journey_warehouse (
    id STRING,
    organization_id STRING,
    user_id STRING,
    session_id STRING,
    touchpoint STRING,
    timestamp TIMESTAMP,
    channel STRING,
    campaign STRING,
    action STRING,
    value FLOAT64,
    created_at TIMESTAMP
);
```

## üîÑ DATA PIPELINE STRATEGY

### **Supabase ‚Üí BigQuery Sync**
```python
# File: backend/app/tasks/data_sync.py

from google.cloud import bigquery
from supabase import create_client
import pandas as pd

def sync_supabase_to_bigquery():
    """Sync data t·ª´ Supabase sang BigQuery"""
    
    # 1. L·∫•y d·ªØ li·ªáu c≈© t·ª´ Supabase
    supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    
    # 2. X√≥a d·ªØ li·ªáu c≈© kh·ªèi Supabase
    def cleanup_old_data():
        # X√≥a hourly_aggregates > 1 ng√†y
        supabase.table('hourly_aggregates').delete().gte('timestamp', '1 day ago').execute()
        
        # X√≥a daily_aggregates > 365 ng√†y
        supabase.table('daily_aggregates').delete().gte('date', '365 days ago').execute()
        
        # X√≥a ai_insights > 90 ng√†y
        supabase.table('ai_insights').delete().gte('created_at', '90 days ago').execute()
    
    # 3. Upload l√™n BigQuery
    def upload_to_bigquery(table_name, data):
        client = bigquery.Client()
        dataset_ref = client.dataset('digital_marketing')
        table_ref = dataset_ref.table(f'{table_name}_historical')
        
        # Convert to DataFrame v√† upload
        df = pd.DataFrame(data)
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
        )
        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()
    
    # 4. Schedule sync
    # Ch·∫°y m·ªói ng√†y l√∫c 02:00
    # Ch·∫°y m·ªói tu·∫ßn cho historical data
```

### **BigQuery ‚Üí Supabase Sync (Khi c·∫ßn)**
```python
# File: backend/app/tasks/bigquery_sync.py

def sync_bigquery_to_supabase():
    """Sync data t·ª´ BigQuery v·ªÅ Supabase khi c·∫ßn"""
    
    client = bigquery.Client()
    
    # Query historical data
    query = """
    SELECT * FROM `project.dataset.daily_aggregates_historical`
    WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    """
    
    df = client.query(query).to_dataframe()
    
    # Upload v·ªÅ Supabase cho analysis
    supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    supabase.table('daily_aggregates').upsert(df.to_dict('records')).execute()
```

## üí∞ COST OPTIMIZATION

### **Supabase Costs**
```bash
# Free Tier (500MB, 2GB bandwidth)
‚úÖ 1000 users: Free
‚úÖ 5000 users: $25/th√°ng (Pro)

# Pro Plan ($25/th√°ng)
‚úÖ 8GB database
‚úÖ 250GB bandwidth
‚úÖ 1000+ organizations
```

### **BigQuery Costs**
```bash
# Storage (Long-term)
‚ùÑÔ∏è $0.02/GB/th√°ng (Standard)
‚ùÑÔ∏è $0.004/GB/th√°ng (Long-term)

# Query Processing
‚ùÑÔ∏è $5/TB processed
‚ùÑÔ∏è 1TB free/th√°ng

# Estimated costs cho 1000 organizations
‚ùÑÔ∏è Storage: ~$10/th√°ng (500GB)
‚ùÑÔ∏è Query: ~$5/th√°ng (1TB processed)
‚ùÑÔ∏è Total: ~$15/th√°ng
```

### **Total Cost Estimation**
```bash
# Phase 1 (MVP - 1000 users)
‚úÖ Supabase Pro: $25/th√°ng
‚ùÑÔ∏è BigQuery: $0/th√°ng (ch∆∞a c·∫ßn)
üìä Total: $25/th√°ng

# Phase 2 (Scale - 10k users)
‚úÖ Supabase Pro: $25/th√°ng
‚ùÑÔ∏è BigQuery: $15/th√°ng
üìä Total: $40/th√°ng

# Phase 3 (Enterprise - 100k users)
‚úÖ Supabase Pro: $25/th√°ng
‚ùÑÔ∏è BigQuery: $50/th√°ng
‚ùÑÔ∏è Advanced Analytics: $100/th√°ng
üìä Total: $175/th√°ng
```

## üîß MIGRATION STRATEGY

### **Phase 1: MVP (Hi·ªán t·∫°i)**
```sql
-- T·∫§T C·∫¢ T·∫†I SUPABASE
‚úÖ Kh√¥ng c·∫ßn BigQuery
‚úÖ Chi ph√≠ th·∫•p: $25/th√°ng
‚úÖ Setup ƒë∆°n gi·∫£n
‚úÖ Performance t·ªët
```

### **Phase 2: Scale (Khi c·∫ßn)**
```sql
-- Th√™m BigQuery khi:
‚ùÑÔ∏è D·ªØ li·ªáu > 8GB
‚ùÑÔ∏è C·∫ßn historical analysis
‚ùÑÔ∏è C·∫ßn advanced analytics
‚ùÑÔ∏è C·∫ßn ML training data
```

### **Migration Scripts**
```python
# File: scripts/migrate_to_bigquery.py

def migrate_to_bigquery():
    """Migrate data t·ª´ Supabase sang BigQuery"""
    
    # 1. Backup current data
    backup_supabase_data()
    
    # 2. Setup BigQuery
    setup_bigquery_tables()
    
    # 3. Sync data
    sync_supabase_to_bigquery()
    
    # 4. Update application config
    update_app_config()
    
    # 5. Test migration
    test_migration()
```

## üìä MONITORING & ALERTS

### **Supabase Monitoring**
```sql
-- Monitor Supabase usage
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

### **BigQuery Monitoring**
```sql
-- Monitor BigQuery usage
SELECT 
    project_id,
    dataset_id,
    table_id,
    size_bytes,
    row_count
FROM `region-us`.INFORMATION_SCHEMA.TABLES
WHERE table_schema = 'digital_marketing'
ORDER BY size_bytes DESC;
```

## üéØ RECOMMENDATIONS

### **‚úÖ B·∫Øt ƒë·∫ßu v·ªõi Supabase**
1. **Setup t·∫•t c·∫£ b·∫£ng t·∫°i Supabase**
2. **Focus v√†o performance v√† user experience**
3. **Monitor usage v√† costs**
4. **Plan migration khi c·∫ßn**

### **‚ùÑÔ∏è Th√™m BigQuery khi:**
1. **D·ªØ li·ªáu > 8GB**
2. **C·∫ßn historical analysis > 1 nƒÉm**
3. **C·∫ßn advanced analytics**
4. **C·∫ßn ML training data**
5. **C√≥ budget cho enterprise features**

### **üîÑ Migration Timeline**
```bash
# Month 1-6: Supabase only
‚úÖ Setup t·∫•t c·∫£ b·∫£ng t·∫°i Supabase
‚úÖ Monitor performance v√† costs
‚úÖ Optimize queries v√† indexes

# Month 6-12: Evaluate BigQuery
‚ùÑÔ∏è Setup BigQuery cho historical data
‚ùÑÔ∏è Implement data sync pipeline
‚ùÑÔ∏è Test hybrid approach

# Month 12+: Full hybrid
‚ùÑÔ∏è Hot data: Supabase
‚ùÑÔ∏è Cold data: BigQuery
‚ùÑÔ∏è Advanced analytics: BigQuery
```

## üìã CHECKLIST

### **Supabase Setup**
- [ ] Core business tables
- [ ] Real-time analytics tables
- [ ] Active insights tables
- [ ] Chart data tables
- [ ] RLS policies
- [ ] Indexes optimization
- [ ] Functions v√† triggers

### **BigQuery Setup (Khi c·∫ßn)**
- [ ] Historical data tables
- [ ] Deep analytics tables
- [ ] Data warehouse tables
- [ ] Data sync pipeline
- [ ] Cost monitoring
- [ ] Performance optimization

### **Migration Planning**
- [ ] Data backup strategy
- [ ] Sync pipeline setup
- [ ] Application config updates
- [ ] Testing procedures
- [ ] Rollback plan

---

**K·∫øt lu·∫≠n: B·∫Øt ƒë·∫ßu v·ªõi Supabase cho t·∫•t c·∫£ b·∫£ng, th√™m BigQuery khi c·∫ßn historical analysis v√† advanced analytics. Chi ph√≠ t·ªëi ∆∞u v√† performance t·ªët cho MVP.** 